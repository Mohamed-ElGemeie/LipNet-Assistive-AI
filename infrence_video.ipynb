{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mgala\\anaconda3\\envs\\lipnet\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from matplotlib import pyplot as plt\n",
    "import imageio\n",
    "from typing import Tuple, Union\n",
    "import math\n",
    "import mediapipe as mp\n",
    "from keras import backend as K\n",
    "import sys\n",
    "import random\n",
    "# from celluloid import Camera \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from IPython.display import display, Image\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixels\n",
    "MARGIN = 0\n",
    "ROW_SIZE = 0\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "\n",
    "# Input model Image size\n",
    "IMAGE_SIZE = (50, 100)\n",
    "\n",
    "batch_size = 1\n",
    "mp_face_mesh =  mp.solutions.face_mesh.FaceMesh(max_num_faces=1,static_image_mode=True, min_detection_confidence=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_to_pixel_coordinates(normalized_x, normalized_y, image_width, image_height):\n",
    "    \"\"\"Converts normalized values to pixel coordinates.\"\"\"\n",
    "    x_px = min(max(int(normalized_x * image_width), 0), image_width)\n",
    "    y_px = min(max(int(normalized_y * image_height), 0), image_height)\n",
    "    return x_px, y_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_based_histogram_equalization_rgb(image):\n",
    "    # # Convert the image to uint8 if not already\n",
    "    # if image.dtype != np.uint8:\n",
    "    #     image = (image * 255).astype(np.uint8)\n",
    "\n",
    "    # Split channels\n",
    "    channels = cv2.split(image)\n",
    "\n",
    "    # Apply histogram equalization to each channel\n",
    "    equalized_channels = [cv2.equalizeHist(channel) for channel in channels]\n",
    "\n",
    "    # Merge channels\n",
    "    equalized_image = cv2.merge(equalized_channels)\n",
    "\n",
    "    return equalized_image\n",
    "\n",
    "def clahe_equalization_rgb(image):\n",
    "    # Convert the image to LAB color space\n",
    "    lab_image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "    # Split LAB image into L, A, B channels\n",
    "    l_channel, a_channel, b_channel = cv2.split(lab_image)\n",
    "\n",
    "    # Apply CLAHE to the L channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))\n",
    "    clahe_l_channel = clahe.apply(l_channel)\n",
    "\n",
    "    # Merge the enhanced L channel with the original A and B channels\n",
    "    clahe_lab_image = cv2.merge((clahe_l_channel, a_channel, b_channel))\n",
    "\n",
    "    # Convert the LAB image back to RGB\n",
    "    clahe_rgb_image = cv2.cvtColor(clahe_lab_image, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "    return clahe_rgb_image\n",
    "\n",
    "def scale_image_0_1(image):\n",
    "\n",
    "    normalized_image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "\n",
    "    return normalized_image\n",
    "\n",
    "def scale_image__1_1(image):\n",
    "\n",
    "    normalized_image = 2 * (image - np.min(image, axis=(0, 1))) / (np.max(image, axis=(0, 1)) - np.min(image, axis=(0, 1))) - 1\n",
    "    normalized_image = np.clip(normalized_image, -1, 1)\n",
    "    \n",
    "    return normalized_image\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    x1, y1 = point1\n",
    "    x2, y2 = point2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "def inner_angle(point1, point2, point3):\n",
    "    a = np.array(point1)\n",
    "    b = np.array(point2)\n",
    "    c = np.array(point3)\n",
    "\n",
    "    ba = a - b\n",
    "    bc = c - b\n",
    "\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(cosine_angle)\n",
    "\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def display_gif(video):\n",
    "    fig, ax = plt.subplots() # make it bigger\n",
    "    camera = Camera(fig)# the camera gets our figure\n",
    "    for img in os.listdir(\"NST/epochs\"):\n",
    "        img_obj = plt.imread(os.path.join(\"NST/epochs\"), img) # reading\n",
    "        ax.imshow(img_obj) # plotting\n",
    "        camera.snap()\n",
    "    animation = camera.animate()\n",
    "    HTML(animation.to_html5_video())\n",
    "    clip = ImageSequenceClip(list(scale_image_0_1(video.numpy())*255), fps=20)\n",
    "    clip.write_gif('test.gif', fps=20)\n",
    "    return Image('test.gif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "def use_camera():\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "    average_angle = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(\"Broken image\")\n",
    "            continue\n",
    "\n",
    "        # Convert to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get face mesh landmarks\n",
    "        results = mp_face_mesh.process(frame_rgb)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                image_rows, image_cols, _ = frame.shape\n",
    "                normalized_point = normalized_to_pixel_coordinates(1,1, image_cols, image_rows)\n",
    "\n",
    "                left_lip = face_landmarks.landmark[0]\n",
    "                right_lip = face_landmarks.landmark[17]\n",
    "                left = face_landmarks.landmark[192]\n",
    "                bottom= face_landmarks.landmark[199]\n",
    "                top = face_landmarks.landmark[2]\n",
    "                right = face_landmarks.landmark[416]\n",
    "\n",
    "                bottom_px = normalized_to_pixel_coordinates(bottom.x, bottom.y, image_cols, image_rows)\n",
    "                left_px = normalized_to_pixel_coordinates(left.x, left.y, image_cols, image_rows)\n",
    "                top_px = normalized_to_pixel_coordinates(top.x, top.y, image_cols, image_rows)\n",
    "                right_px = normalized_to_pixel_coordinates(right.x, right.y, image_cols, image_rows)\n",
    "                left_lip = normalized_to_pixel_coordinates(left_lip.x, left_lip.y, image_cols, image_rows)\n",
    "                right_lip = normalized_to_pixel_coordinates(right_lip.x, right_lip.y, image_cols, image_rows)\n",
    "\n",
    "\n",
    "                midpoint = [(left_lip[0]+right_lip[0])//2,(left_lip[1]+right_lip[1])//2]\n",
    "\n",
    "                # Calculate the differences in x and y coordinates\n",
    "                dx = right_lip[0] - midpoint[0]\n",
    "                dy = right_lip[1] - midpoint[1]\n",
    "                \n",
    "                # # Calculate the angle using arctan(dy/dx)\n",
    "                angle_radians = math.atan2(dy, dx)\n",
    "                \n",
    "                # # Convert radians to degrees\n",
    "                angle_degrees = math.degrees(angle_radians) - 90\n",
    "\n",
    "                if len(average_angle) <6:\n",
    "                    average_angle.append(angle_degrees)\n",
    "                else:\n",
    "                    average_angle.pop(0)\n",
    "                    average_angle.append(angle_degrees)\n",
    "                    \n",
    "                if sum(average_angle) == 0:\n",
    "                    angle_degrees = 0\n",
    "                else:\n",
    "                    angle_degrees = sum(average_angle) / len(average_angle)\n",
    "\n",
    "                # # Rotate the image\n",
    "                rotation_matrix = cv2.getRotationMatrix2D(midpoint, angle_degrees, 1.0)\n",
    "                frame = cv2.warpAffine(frame, rotation_matrix, (normalized_point))\n",
    "\n",
    "                # Convert rotated points back to list of tuples\n",
    "                bottom_px , left_px , top_px , right_px = [(int(point[0][0]), int(point[0][1])) for point in cv2.transform(np.array([bottom_px , left_px , top_px , right_px]).reshape(-1, 1, 2).astype(np.float64), rotation_matrix)]\n",
    "\n",
    "                if not (frame.shape[0] >0 and frame.shape[1] >0 and frame.shape[2] ==3):\n",
    "                    continue\n",
    "\n",
    "                frame = frame[\n",
    "                        top_px[1] : bottom_px[1],\n",
    "                        left_px[0] : right_px[0],\n",
    "                        :,\n",
    "                    ]\n",
    "                \n",
    "                if not (frame.shape[0] >0 and frame.shape[1] >0 and frame.shape[2] ==3):\n",
    "                    continue\n",
    "\n",
    "                frame = cv2.resize(\n",
    "                        frame, (IMAGE_SIZE[1],IMAGE_SIZE[0]), interpolation=cv2.INTER_LANCZOS4\n",
    "                    )\n",
    "                \n",
    "                frame = clahe_equalization_rgb(frame)\n",
    "\n",
    "                mean = np.mean(frame, axis=(0, 1))  \n",
    "                std = np.std(frame, axis=(0, 1))    \n",
    "                frame = (frame - mean) / std\n",
    "                cv2.imshow(\"frame\", scale_image_0_1 (frame))\n",
    "                break\n",
    "\n",
    "        # Display the frame\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# use_camera()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LETTERS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ابتثجحخدذرزسشصضطظعغفقكلمنهويىء٠١٢٣٤٥٦٧٨٩'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB =\"ابتثجحخدذرزسشصضطظعغفقكلمنهويىء٠١٢٣٤٥٦٧٨٩\"\n",
    "VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set() set()\n"
     ]
    }
   ],
   "source": [
    "set1 = set(VOCAB.replace(\"\\n\",\"\"))\n",
    "set2 = set(\"ابتثجحخدذرزسشصضطظعغفقكلمنهويىء٠١٢٣٤٥٦٧٨٩\")\n",
    "common_chars = set1.intersection(set2)\n",
    "difference_set1 = set1.difference(set2)\n",
    "difference_set2 = set2.difference(set1)\n",
    "\n",
    "print(difference_set1, difference_set2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_FRAMES = 138\n",
    "MAX_FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [x for x in \"ابتثجحخدذرزسشصضطظعغفقكلمنهويىء٠١٢٣٤٥٦٧٨٩ \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary is: ['', 'ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'ى', 'ء', '٠', '١', '٢', '٣', '٤', '٥', '٦', '٧', '٨', '٩', ' '] (size =42)\n"
     ]
    }
   ],
   "source": [
    "char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\n",
    "num_to_char = tf.keras.layers.StringLookup(\n",
    "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
    "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_to_arab(alignments):\n",
    "    sub_arrays = []\n",
    "    current_sub_array = []\n",
    "    for element in num_to_char(alignments).numpy():\n",
    "        if element.decode(\"utf-8\") == \"\":\n",
    "            if current_sub_array:  # Add a non-empty sub-array\n",
    "                sub_arrays.append(current_sub_array)\n",
    "            current_sub_array = []  # Start a new sub-array\n",
    "        else:\n",
    "            current_sub_array.append(element.decode(\"utf-8\"))\n",
    "\n",
    "    if current_sub_array:  # Add the last sub-array if it's not empty\n",
    "        sub_arrays.append(current_sub_array)\n",
    "\n",
    "    for i in sub_arrays:\n",
    "        print(''.join(i),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProduceExample(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.sample = train_data.as_numpy_iterator().next()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None) -> None:\n",
    "        data = self.sample\n",
    "        yhat = self.model.predict(data[0],verbose=0)  # [75,75]\n",
    "\n",
    "        decoded_200 = tf.keras.backend.ctc_decode(yhat, [MAX_FRAMES], greedy=False,beam_width=200)[0][0].numpy()\n",
    "        decoded_100 = tf.keras.backend.ctc_decode(yhat, [MAX_FRAMES], greedy=False,beam_width=100)[0][0].numpy()\n",
    "        decoded_50 = tf.keras.backend.ctc_decode(yhat, [MAX_FRAMES], greedy=False,beam_width=50)[0][0].numpy()\n",
    "        decoded_letter = tf.keras.backend.ctc_decode(yhat, [MAX_FRAMES], greedy=False,beam_width=char_to_num.vocabulary_size()+1)[0][0].numpy()\n",
    "        decoded_40 = tf.keras.backend.ctc_decode(yhat, [MAX_FRAMES], greedy=False,beam_width=40)[0][0].numpy()\n",
    "        decoded_30 = tf.keras.backend.ctc_decode(yhat, [MAX_FRAMES], greedy=False,beam_width=30)[0][0].numpy()\n",
    "        decoded_20 = tf.keras.backend.ctc_decode(yhat, [MAX_FRAMES], greedy=False,beam_width=20)[0][0].numpy()\n",
    "        decoded_10 = tf.keras.backend.ctc_decode(yhat, [MAX_FRAMES], greedy=False,beam_width=10)[0][0].numpy()\n",
    "        decoded_5 = tf.keras.backend.ctc_decode(yhat, [MAX_FRAMES], greedy=False,beam_width=5)[0][0].numpy()\n",
    "        decoded_1 = tf.keras.backend.ctc_decode(yhat, [MAX_FRAMES], greedy=False,beam_width=1)[0][0].numpy()\n",
    "        decoded_1_greedy = tf.keras.backend.ctc_decode(yhat, [MAX_FRAMES], greedy=True)[0][0].numpy()\n",
    "\n",
    "        for x in range(len(yhat)):\n",
    "            print(\"\\n\\n\")\n",
    "            print(\"Original:\",tf.strings.reduce_join(num_to_char(data[1][x])).numpy().decode(\"utf-8\"))\n",
    "            print(\"Prediction Dec 200:\",tf.strings.reduce_join(num_to_char(decoded_200[x])).numpy().decode(\"utf-8\"))\n",
    "            print(\"Prediction Dec 100:\",tf.strings.reduce_join(num_to_char(decoded_100[x])).numpy().decode(\"utf-8\"))\n",
    "            print(\"Prediction Dec 50:\",tf.strings.reduce_join(num_to_char(decoded_50[x])).numpy().decode(\"utf-8\"))\n",
    "            print(\"Prediction Dec Voc:\",tf.strings.reduce_join(num_to_char(decoded_letter[x])).numpy().decode(\"utf-8\"))\n",
    "            print(\"Prediction Dec 40:\",tf.strings.reduce_join(num_to_char(decoded_40[x])).numpy().decode(\"utf-8\"))\n",
    "            print(\"Prediction Dec 30:\",tf.strings.reduce_join(num_to_char(decoded_30[x])).numpy().decode(\"utf-8\"))\n",
    "            print(\"Prediction Dec 20:\",tf.strings.reduce_join(num_to_char(decoded_20[x])).numpy().decode(\"utf-8\"))\n",
    "            print(\"Prediction Dec 10:\",tf.strings.reduce_join(num_to_char(decoded_10[x])).numpy().decode(\"utf-8\"))\n",
    "            print(\"Prediction Dec 5:\",tf.strings.reduce_join(num_to_char(decoded_5[x])).numpy().decode(\"utf-8\"))\n",
    "            print(\"Prediction Dec 1:\",tf.strings.reduce_join(num_to_char(decoded_1[x])).numpy().decode(\"utf-8\"))\n",
    "            print(\"Prediction Dec Greedy:\",tf.strings.reduce_join(num_to_char(decoded_1_greedy[x])).numpy().decode(\"utf-8\"))\n",
    "            print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path: str) -> List[float]:\n",
    "    # opens the video's path as a camera object\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    # frames of the video\n",
    "    frames = np.empty((MAX_FRAMES, IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "    average_angle = []\n",
    "    idx = 0\n",
    "\n",
    "\n",
    "    # loop over all frames in the video\n",
    "    for _ in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "        # if _ == 0:\n",
    "        #     continue\n",
    "        try:\n",
    "            ret, frame = cap.read()\n",
    "            image_rows, image_cols, _ = frame.shape\n",
    "            # convert to rgb\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # get the keypoints (x,y)s of each point of a face in the frame\n",
    "            face_mesh_result = mp_face_mesh.process(frame)\n",
    "\n",
    "            # loop over each face landmark ~ (nose (x,y) lips (x,y) and so one and a bounding box)\n",
    "            if face_mesh_result.multi_face_landmarks:\n",
    "\n",
    "                for face_landmarks in face_mesh_result.multi_face_landmarks:\n",
    "\n",
    "                    normalized_point = normalized_to_pixel_coordinates(1,1, image_cols, image_rows)\n",
    "\n",
    "                    left_lip = face_landmarks.landmark[0]\n",
    "                    right_lip = face_landmarks.landmark[17]\n",
    "                    left = face_landmarks.landmark[192]\n",
    "                    bottom= face_landmarks.landmark[199]\n",
    "                    top = face_landmarks.landmark[2]\n",
    "                    right = face_landmarks.landmark[416]\n",
    "\n",
    "                    bottom_px = normalized_to_pixel_coordinates(bottom.x, bottom.y, image_cols, image_rows)\n",
    "                    left_px = normalized_to_pixel_coordinates(left.x, left.y, image_cols, image_rows)\n",
    "                    top_px = normalized_to_pixel_coordinates(top.x, top.y, image_cols, image_rows)\n",
    "                    right_px = normalized_to_pixel_coordinates(right.x, right.y, image_cols, image_rows)\n",
    "                    left_lip = normalized_to_pixel_coordinates(left_lip.x, left_lip.y, image_cols, image_rows)\n",
    "                    right_lip = normalized_to_pixel_coordinates(right_lip.x, right_lip.y, image_cols, image_rows)\n",
    "\n",
    "\n",
    "                    midpoint = [(left_lip[0]+right_lip[0])//2,(left_lip[1]+right_lip[1])//2]\n",
    "\n",
    "                    # Calculate the differences in x and y coordinates\n",
    "                    dx = right_lip[0] - midpoint[0]\n",
    "                    dy = right_lip[1] - midpoint[1]\n",
    "                    \n",
    "                    # # Calculate the angle using arctan(dy/dx)\n",
    "                    angle_radians = math.atan2(dy, dx)\n",
    "                    \n",
    "                    # # Convert radians to degrees\n",
    "                    angle_degrees = math.degrees(angle_radians) - 90\n",
    "\n",
    "                    if len(average_angle) <6:\n",
    "                        average_angle.append(angle_degrees)\n",
    "                    else:\n",
    "                        average_angle.pop(0)\n",
    "                        average_angle.append(angle_degrees)\n",
    "                        \n",
    "                    if sum(average_angle) == 0:\n",
    "                        angle_degrees = 0\n",
    "                    else:\n",
    "                        angle_degrees = sum(average_angle) / len(average_angle)\n",
    "\n",
    "                    # Rotate the image\n",
    "                    rotation_matrix = cv2.getRotationMatrix2D(midpoint, angle_degrees, 1.0)\n",
    "                    frame = cv2.warpAffine(frame, rotation_matrix, (normalized_point))\n",
    "\n",
    "                    # Convert rotated points back to list of tuples\n",
    "                    bottom_px , left_px , top_px , right_px = [(int(point[0][0]), int(point[0][1])) for point in cv2.transform(np.array([bottom_px , left_px , top_px , right_px]).reshape(-1, 1, 2).astype(np.float64), rotation_matrix)]\n",
    "\n",
    "                    if not (frame.shape[0] >0 and frame.shape[1] >0 and frame.shape[2] ==3):\n",
    "                        continue\n",
    "\n",
    "                    frame = frame[\n",
    "                            top_px[1] : bottom_px[1],\n",
    "                            left_px[0] : right_px[0],\n",
    "                            :,\n",
    "                        ]\n",
    "                    \n",
    "                    if not (frame.shape[0] >0 and frame.shape[1] >0 and frame.shape[2] ==3):\n",
    "                        continue\n",
    "\n",
    "                    frame = cv2.resize(\n",
    "                            frame, (IMAGE_SIZE[1],IMAGE_SIZE[0]), interpolation=cv2.INTER_LANCZOS4\n",
    "                        )\n",
    "                    \n",
    "                    frame = clahe_equalization_rgb(frame)\n",
    "\n",
    "                    mean = np.mean(frame, axis=(0, 1))  \n",
    "                    std = np.std(frame, axis=(0, 1))    \n",
    "                    frame = (frame - mean) / std\n",
    "\n",
    "                    frames[idx] = frame\n",
    "\n",
    "                    idx += 1\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "                # Handle the case where the landmarks are not found\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "    cap.release()\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "    # normalize on the scale of the video\n",
    "    mean = tf.math.reduce_mean(frames)\n",
    "    std = tf.math.reduce_std(tf.cast(frames, tf.float32))\n",
    "    frames = tf.cast((frames - mean), tf.float32) / std\n",
    "\n",
    "    if idx < MAX_FRAMES:\n",
    "        for i in range(idx, MAX_FRAMES):\n",
    "            frames = tf.tensor_scatter_nd_update(frames, [[i]], tf.zeros((1, IMAGE_SIZE[0], IMAGE_SIZE[1], 3), dtype=tf.float32))\n",
    "\n",
    "    # frames = tf.cast(tf.convert_to_tensor(frames),tf.float32)\n",
    "    \n",
    "    return frames.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alignments(path:str) -> List[str]: \n",
    "    with open(path, 'r') as f: \n",
    "        lines = f.readlines() \n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        line = line.strip(\"\\n\")\n",
    "        tokens = [*tokens,' ',line]\n",
    "\n",
    "    return char_to_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), (-1)))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential ,model_from_json\n",
    "from tensorflow.keras.layers import Conv3D,GRU, Dense, Dropout, Bidirectional, MaxPool3D, Activation, TimeDistributed, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv3D(32,(1, 5, 5),strides=(1,2,2), input_shape=(MAX_FRAMES, IMAGE_SIZE[0],IMAGE_SIZE[1], 3), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "model.add(Conv3D(64,(1, 5, 5), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "model.add(Conv3D(96,(1, 3, 3), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool3D((1,2,2)))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(Bidirectional(GRU(256,kernel_initializer=keras.initializers.Orthogonal(), return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(GRU(256,kernel_initializer=keras.initializers.Orthogonal(), return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(char_to_num.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax'))\n",
    "model.load_weights(r\"E:/37/test1.weights.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step\n",
      "ده كانا مي في الشغل الايه ما بي هو\n",
      "ده كان زميلي في الشغل لغايه ما بعني هو None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
      "تني\n",
      "اتنين None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "وزير الصحه بقى فضي يقال اي حد كده وانت\n",
      "وزير الصحه بقى فاضي يقابل اي حد كده وانت None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
      "الافواه واضطاده الصحين المعارضين\n",
      "الافواه واضطهاد الصحفيين المعارضين None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step\n",
      "الو بت الي\n",
      "الو نصبت الاستيج None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step\n",
      "خلص ايه ديه يرحا الع طب عره ما كان\n",
      "خلاص ايه ده يا رحاب الع طب عمره ما كان None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 255ms/step\n",
      "الي الار بتعله لو طفل ما اتاصا او\n",
      "اللي الدار بتعمله لو طفل مثلا اتصاب او None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step\n",
      "ويا بخت كل طفل هن بام زي حضتك بس كان\n",
      "ويا بخت كل طفل هنا بام زي حضرتك بس كان None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\n",
      "ده مو في الدولا بتعه فحبيتا عرف\n",
      "ده موجود في الدولاب بتاعها فحبيت اعرف None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step\n",
      "شهات مل من واره الصحه ويسل\n",
      "شهادات ميلاد من وزاره الصحه وبيسجل None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n",
      "اله فلضل حر\n",
      "الله افضل حضرتك None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step\n",
      "ان قل هو قله كن ورا الحفه الي\n",
      "انا هقوللك هو قصله كان ورا الحفله اللي None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step\n",
      "اترفع فيه علم ان\n",
      "اترفع فيها عالم الناس None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "ملجه طيب مش عارفه تعرفي من ابوك اليقي\n",
      "ملجه طيب مش عارفه تعرفي مين ابوك الحقيقي None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n",
      "شهب حمد بيترف فيه انه بلى مول ل من\n",
      "شهاب حامد بيعترف فيه انه بلاقى تمويل من None\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step\n",
      "نمع التجي لي خصا بيه الاتا لال\n",
      "هنسمع التسجيل اللي خصنا بيه الاستاذ هلال None\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "for idx,i in enumerate(glob.glob(r\"E:/Video Links Dataset/filtered/videos_25fps/*.mp4\")):  \n",
    "    i = i.replace(\"\\\\\",\"/\") \n",
    "    x = load_video(i.replace(\"\\\\\",\"/\"))\n",
    "    model_pred = model.predict([x.reshape((1,*x.shape))])\n",
    "    decoded_pred = tf.keras.backend.ctc_decode(model_pred, [MAX_FRAMES], greedy=False,beam_width=200)[0][0].numpy()\n",
    "    decoded_text = tf.strings.reduce_join(num_to_char(decoded_pred[0])).numpy().decode(\"utf-8\")\n",
    "    print(decoded_text)\n",
    "    print(char_to_arab(load_alignments(fr\"E:/Video Links Dataset/filtered/alignments/{i.split('/')[-1].split('.')[0]}.align\").numpy()))\n",
    "    print(\"\\n\\n\\n\")\n",
    "    if idx == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27, 23,  1, 41, 24,  1, 41,  1, 23, 28, 41, 18, 23, 18, 41, 18,\n",
       "        23,  1, 41,  1, 18, 41,  1, 25, 41,  1,  2,  0, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ولا ما الي علع علا اع ان اب'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = tf.strings.reduce_join(num_to_char(decoded_pred[0])).numpy().decode(\"utf-8\")\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بعد اذنك انا عايز اعدى "
     ]
    }
   ],
   "source": [
    "char_to_arab(load_alignments(r\"E:/data/me/alignments/18 Galal.align\").numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class CERMetric(tf.keras.metrics.Metric):\n",
    "    \"\"\"\n",
    "    A custom Keras metric to compute the Character Error Rate\n",
    "    \"\"\"\n",
    "    def __init__(self, name='CER_metric', **kwargs):\n",
    "        super(CERMetric, self).__init__(name=name, **kwargs)\n",
    "        self.cer_accumulator = self.add_weight(name=\"total_cer\", initializer=\"zeros\")\n",
    "        self.counter = self.add_weight(name=\"cer_count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "\n",
    "        input_shape = K.shape(y_pred)\n",
    "        input_length = tf.ones(shape=input_shape[0], dtype=tf.int64) * K.cast(input_shape[1], tf.int64)\n",
    "\n",
    "        decode, log = K.ctc_decode(y_pred,\n",
    "                                    input_length,\n",
    "                                    greedy=True)\n",
    "\n",
    "        decode = K.ctc_label_dense_to_sparse(decode[0], K.cast(input_length, tf.int32))\n",
    "        y_true_sparse = K.ctc_label_dense_to_sparse(y_true, K.cast(input_length, tf.int32))\n",
    "\n",
    "        decode = tf.sparse.retain(decode, tf.not_equal(decode.values, -1))\n",
    "        distance = tf.edit_distance(decode, y_true_sparse, normalize=True)\n",
    "\n",
    "        self.cer_accumulator.assign_add(tf.reduce_sum(distance))\n",
    "        self.counter.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return tf.math.divide_no_nan(self.cer_accumulator, self.counter)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.cer_accumulator.assign(0.0)\n",
    "        self.counter.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calc_WER(y_true, y_pred):\n",
    "    for word in y_true.split():\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wer \u001b[38;5;241m=\u001b[39m \u001b[43mWERMetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_alignments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE:/data/me/alignments/18 Galal.align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoded_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "wer = WERMetric(\n",
    "    load_alignments(r\"E:/data/me/alignments/18 Galal.align\").numpy(),\n",
    "                decoded_pred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer.update_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "بعد\n",
    "اذنك\n",
    "انا\n",
    "عايز\n",
    "اعدى"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lipnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
